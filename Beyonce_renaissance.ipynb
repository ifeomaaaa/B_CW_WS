{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beyonce renaissance.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOiN4s3kuWIYfVJ52PeycdQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWy8hHVfBkaZ",
        "outputId": "89ca55ed-fa9a-4958-eb17-bc58dbfd4d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import tweepy as tw\n",
        "from textblob import TextBlob\n",
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud\n",
        "import json\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "consumer_key= 'insert'\n",
        "consumer_secret= 'insert'\n",
        "access_token= 'insert'\n",
        "access_token_secret= 'insert'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
      ],
      "metadata": {
        "id": "ISKGT0jhCNjC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "fZvEqAy6SZNx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_tweets = api.search('Beyonce renaissance -filter:retweets', count=1000)\n",
        "public_tweets = api.search('#commonwealthgames2022 -filter:retweets', count=1000)\n",
        "public_tweets = api.search('Will Smith -filter:retweets', count=1000)\n",
        "\n",
        "StartDate = input(\"Enter the start date in this format yyyy-mm-dd: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgH_hRSsCSfk",
        "outputId": "81635e3a-a369-46c7-eefb-800bd1150359"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the start date in this format yyyy-mm-dd: 2022-07-29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Dataframe of Tweets\n",
        "#Cleaning searched tweets and converting into Dataframe\n",
        "my_list_of_dicts = []\n",
        "for each_json_tweet in public_tweets:\n",
        "    my_list_of_dicts.append(each_json_tweet._json)\n",
        "\n",
        "    \n",
        "with open('tweet_json_Data.txt', 'w') as file:\n",
        "        file.write(json.dumps(my_list_of_dicts, indent=4))\n",
        "\n",
        "        \n",
        "my_demo_list = []\n",
        "with open('tweet_json_Data.txt', encoding='utf-8') as json_file:  \n",
        "    all_data = json.load(json_file)\n",
        "    for each_dictionary in all_data:\n",
        "        tweet_id = each_dictionary['id']\n",
        "        text = each_dictionary['text']\n",
        "        favorite_count = each_dictionary['favorite_count']\n",
        "        retweet_count = each_dictionary['retweet_count']\n",
        "        created_at = each_dictionary['created_at']\n",
        "        my_demo_list.append({'tweet_id': str(tweet_id),\n",
        "                             'text': str(text),\n",
        "                             'favorite_count': int(favorite_count),\n",
        "                             'retweet_count': int(retweet_count),\n",
        "                             'created_at': created_at,\n",
        "                            })\n",
        "        \n",
        "        tweet_dataset = pd.DataFrame(my_demo_list, columns = \n",
        "                                  ['tweet_id', 'text', \n",
        "                                   'favorite_count', 'retweet_count', \n",
        "                                   'created_at'])\n",
        "\n",
        "\n",
        " #Writing tweet dataset ti csv file for future reference\n",
        "tweet_dataset.to_csv('Beyonce.csv')"
      ],
      "metadata": {
        "id": "cKscW4D4Cvhb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanUpTweet(txt):\n",
        "  txt = re.sub(r'@[A-Za-z0-9_]+', '',txt)\n",
        "  txt = re.sub(r'#', '', txt)\n",
        "  txt = re.sub(r'RT : ', '', txt)\n",
        "  txt = re.sub(r'https?:\\/\\/[A-Aa-a0-9\\.\\/]+', '', txt)\n",
        "  return txt"
      ],
      "metadata": {
        "id": "ce38JW77MX2L"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_dataset.head(10)"
      ],
      "metadata": {
        "id": "gZuyrBKgC4FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#attempting to add columns for names mentioned\n",
        "def get_keywords(row):\n",
        "    keywords = []\n",
        "    text = row[\"text\"].lower()\n",
        "    if \"Beyonce\" in text or \"renaissance\" in text:\n",
        "        keywords.append(\"Beyonce\")\n",
        "    if \"Will Smith\" in text or \"Slap\" in text:\n",
        "        keywords.append(\"Will\")\n",
        "    if \"commonwealth\" in text or \"games\" in text:\n",
        "        keywords.append(\"CW22\")\n",
        "    return \",\".join(keywords)\n",
        "tweet_dataset[\"keyword\"] = tweet_dataset.apply(get_keywords,axis=1)"
      ],
      "metadata": {
        "id": "hLButznIIyu4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jGDmpYAhhB12"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = tweet_dataset[\"keyword\"].value_counts()\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWOzLoisEbQd",
        "outputId": "a68645b6-4d61-4fac-b391-ed9886750c54"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    100\n",
            "Name: keyword, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}